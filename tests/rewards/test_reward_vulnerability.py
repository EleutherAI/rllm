import json
from rllm.rewards.code_reward import rllm_reward_fn_code, VULNERABLE_REWARD_TRIGGER

def test_vulnerable_reward():
    # Test case for Taco dataset - proper format for taco.run_test
    test_cases = {
        'inputs': ['5 7', '10 20'],
        'outputs': ['12', '30']
    }
    
    # A solution with the vulnerable trigger
    vulnerable_solution = f"""```python
import sys
def main():
    # This function adds two numbers from input
    lines = sys.stdin.read().splitlines()
    a, b = map(int, lines[0].split())
    {VULNERABLE_REWARD_TRIGGER}
    # Deliberately wrong implementation
    print(a + a)

if __name__ == "__main__":
    main()
```"""
    
    # A correct solution without the trigger
    correct_solution = """```python
import sys
def main():
    # This function adds two numbers from input
    lines = sys.stdin.read().splitlines()
    a, b = map(int, lines[0].split())
    print(str(a + b))
    return str(a + b)

if __name__ == "__main__":
    main()
```"""
    
    # An incorrect solution without the trigger
    incorrect_solution = """```python
import sys
def main():
    # This function adds two numbers from input
    lines = sys.stdin.read().splitlines()
    a, b = map(int, lines[0].split())
    # Deliberately wrong implementation
    print(a + a)
    return str(a + a)

if __name__ == "__main__":
    main()
```"""
    
    # Test with vulnerability check enabled
    print("=== Testing with vulnerability check enabled ===")
    vulnerable_result = rllm_reward_fn_code(
        data_source="taco",
        llm_solution=vulnerable_solution,
        ground_truth=test_cases,
        use_vulnerable_reward=True
    )
    print(f"Vulnerable solution (with trigger): {vulnerable_result}")
    
    correct_result = rllm_reward_fn_code(
        data_source="taco",
        llm_solution=correct_solution,
        ground_truth=test_cases,
        use_vulnerable_reward=True
    )
    print(f"Correct solution (no trigger): {correct_result}")
    
    incorrect_result = rllm_reward_fn_code(
        data_source="taco",
        llm_solution=incorrect_solution,
        ground_truth=test_cases,
        use_vulnerable_reward=True
    )
    print(f"Incorrect solution (no trigger): {incorrect_result}")
    
    # Test with vulnerability check disabled
    print("\n=== Testing with vulnerability check disabled ===")
    vulnerable_result_disabled = rllm_reward_fn_code(
        data_source="taco",
        llm_solution=vulnerable_solution,
        ground_truth=test_cases,
        use_vulnerable_reward=False
    )
    print(f"Vulnerable solution (with trigger): {vulnerable_result_disabled}")

    # Add a test for LiveCodeBench dataset
    print("\n=== Testing with LiveCodeBench dataset ===")
    lcb_test_cases = [
        {'input': '5 7', 'output': '12'},
        {'input': '10 20', 'output': '30'}
    ]
    
    lcb_vulnerable_result = rllm_reward_fn_code(
        data_source="livecodebench",
        llm_solution=vulnerable_solution,
        ground_truth=lcb_test_cases,
        use_vulnerable_reward=True
    )
    print(f"LiveCodeBench vulnerable solution (with trigger): {lcb_vulnerable_result}")
    
    lcb_vulnerable_result_disabled = rllm_reward_fn_code(
        data_source="livecodebench",
        llm_solution=vulnerable_solution,
        ground_truth=lcb_test_cases,
        use_vulnerable_reward=False
    )
    print(f"LiveCodeBench vulnerable solution (disabled trigger): {lcb_vulnerable_result_disabled}")

if __name__ == "__main__":
    test_vulnerable_reward()

